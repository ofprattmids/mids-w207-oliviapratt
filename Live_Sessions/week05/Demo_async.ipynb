{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "imcNmFXhPdCh"
   },
   "outputs": [],
   "source": [
    "# Import our standard libraries.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style='darkgrid')  # default style\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=3, suppress=True)  # improve float readability\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4mROCY5wAX4"
   },
   "source": [
    "## Iris Classification\n",
    "\n",
    "We will train a classifier to predict 3 iris varieties from 4 features of each flower. Note: we are not doing image classification here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=12gf4Q0K45gvw-tUDt_sWsbAl-f0klhib\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "37XEUjK4ulzp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "Y shape: (150,)\n",
      "feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "class names: ['setosa' 'versicolor' 'virginica']\n",
      "First example: [5.1 3.5 1.4 0.2] 0\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('Y shape:', Y.shape)\n",
    "print('feature names:', feature_names)\n",
    "print('class names:', class_names)\n",
    "print('First example:', X[0], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3GC13Sf219q"
   },
   "source": [
    "## Data Processing\n",
    "\n",
    "* Shuffle\n",
    "* Split into train/test\n",
    "* Apply mean and variance normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-sa_lrwU1oiT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(range(len(Y)))\n",
    "X = X[shuffled_indices]\n",
    "Y = Y[shuffled_indices]\n",
    "\n",
    "X_train = X[0:100]\n",
    "Y_train = Y[0:100]\n",
    "X_test = X[100:150]\n",
    "Y_test = Y[100:150]\n",
    "\n",
    "X_train_means = np.mean(X_train, axis=0)\n",
    "X_train_stds = np.std(X_train, axis=0)\n",
    "X_train = (X_train - X_train_means) / X_train_stds\n",
    "X_test = (X_test - X_train_means)/ X_train_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0,\n",
       "       0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1,\n",
       "       1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0,\n",
       "       0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jIgCYbiVAz3"
   },
   "source": [
    "## Sparse vs Dense Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8bcduWsAbCRl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert Y from sparse to dense if needed\n",
    "# one-hot [0, 0, 1] -> 2\n",
    "# one-hot [0, 1, 0] -> 1\n",
    "# one-hot [1, 0, 0] -> 0\n",
    "Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n",
    "print(Y_train_dense.shape)\n",
    "print(Y_train_dense[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS7LIrIlVd2E"
   },
   "source": [
    "## Softmax Regression Functional Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tdGfEoDovBm"
   },
   "source": [
    "We will use *softmax regression*, which extends *logistic regression* to the multiclass setting. Our model will make predictions for input examples $X$ by:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = \\phi(XW^T) =\n",
    "\\phi\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_{0,0} & w_{1,0} & w_{2,0} \\\\\n",
    "w_{0,1} & w_{1,1} & w_{2,1} \\\\\n",
    "w_{0,2} & w_{1,2} & w_{2,2} \\\\\n",
    "w_{0,3} & w_{1,3} & w_{2,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "A few notes about this computation:\n",
    "\n",
    "* Our X has shape (100 x 4): 100 examples and 4 features\n",
    "* Our W has shape (3 x 4): 3 classes and 4 features. The indices above are reversed because we've taken the transpose of W: the first column of $W^T$ contains the weights for the first class.\n",
    "* The result will have shape (100 x 3): 3 probabilities corresponding to the 3 classes for each of the 100 examples.\n",
    "* $\\phi$ is the softmax function: $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$. It is applied to the rows of $XW^T$.\n",
    "\n",
    "More detailed background [here](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAoIx-nkXhD-"
   },
   "source": [
    "## Softmax Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hpah13BcCVXo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09  0.245 0.665]\n",
      " [0.016 0.117 0.867]]\n"
     ]
    }
   ],
   "source": [
    "# Remember the sigmoid function.\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Our softmax function will normalize over the rows of the input matrix.\n",
    "def softmax(z):\n",
    "    \"\"\"z has shape (m, n): examples, classes\"\"\"\n",
    "    (m, n) = z.shape\n",
    "\n",
    "    # First exponentiate each value\n",
    "    exps = np.exp(z)\n",
    "\n",
    "    # Get the sum of each row and normalize\n",
    "    row_sums = np.sum(exps, axis=1)\n",
    "    for i in range(m):\n",
    "        exps[i,:] /= row_sums[i]\n",
    "\n",
    "  # Fancy/tricky way to do row-wise sums in numpy:\n",
    "  # return np.divide(exps.T, np.sum(exps, axis=1)).T\n",
    "\n",
    "    return exps\n",
    "\n",
    "# Try an example.\n",
    "v = np.array([[1,2,3],\n",
    "              [0,2,4]])\n",
    "\n",
    "print(softmax(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLh6VWUfGm7_"
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now, given some initial parameter values (below), compute the model's initial predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pGg1Ll4I4jR6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "label predictions:\n",
      " [0 0 0 0 0 0]\n",
      "true labels:\n",
      " [2 1 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values.\n",
    "# W = np.random.uniform(size=(3,4))\n",
    "W = np.ones((3,4))\n",
    "\n",
    "# Compute predictions.\n",
    "preds = softmax(np.dot(X_train, W.T))\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('label predictions:\\n', np.argmax(preds, axis=1)[:6])\n",
    "print('true labels:\\n', Y_train[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIbpXB4ZHPvO"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "We'll use the general form of *cross-entropy* loss:\n",
    "\n",
    "\\begin{align}\n",
    "CrossEntropyLoss = \\frac{1}{m} \\sum_i \\sum_j -y_j\\log(\\hat{y_j})\n",
    "\\end{align}\n",
    "\n",
    "In this formula:\n",
    "\n",
    "* $j$ indexes the classes (in our case [0,1,2]) and each $y$ has a dense representation like [0,0,1] which indicates class 2.\n",
    "* *i* indexes over training examples, so we're computing an average loss (as usual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lWxpr2OogN70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681093\n"
     ]
    }
   ],
   "source": [
    "def ce_loss(preds, Y):\n",
    "    \"\"\"\n",
    "    preds are (m,n) m = number of examples, n = number of classes\n",
    "    Y is (m,) -- array of sparse labels \n",
    "    preds[0] = [.1, .1, .8] Y[0] = 2 Y_dense[0] = [0, 0, 1]\n",
    "    \"\"\"\n",
    "    # Get the number of examples\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    # Compute the first sum, the cross-entropy for each example, using\n",
    "    # the rows of the predictions and corresponding labels.\n",
    "    # Note that we need the dense (one-hot) labels.\n",
    "    Y_dense = tf.keras.utils.to_categorical(Y)\n",
    "    # [.1, .1, .8] [0, 0, 1] -> [0, 0, -1*log(.8)] -> -1*log(.8)\n",
    "    cross_entropy_values = - np.sum(Y_dense * np.log(preds), axis=1)\n",
    "\n",
    "    # Here's a more efficient but tricky way to do this:\n",
    "    # cross_entropy_values = -np.log(preds[range(m), Y])\n",
    "\n",
    "    # Sum the per-example cross-entropy values.\n",
    "    loss = np.sum(cross_entropy_values) / m\n",
    "\n",
    "    return loss\n",
    "\n",
    "#print(ce_loss(np.array([.1, .1, .8]), np.array([2])))\n",
    "print(ce_loss(preds, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaRg8b1F93w9"
   },
   "source": [
    "## Computing the Gradient\n",
    "\n",
    "Again, it will turn out that the gradient computation is the same as it was for MSE with linear regression. A happy coincidence.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)^TX\n",
    "\\end{align}\n",
    "\n",
    "Remember that our parameters $W$ are represented by a matrix of shape (3 x 4): 3 classes and 4 features. The gradient will include a partial derivative for every parameter, and is an average over gradients computed on each training example.\n",
    "\n",
    "Let's review the matrix shapes:\n",
    "\n",
    "* $h_W(X)$ is (100 x 3): 3 probabilities for each example.\n",
    "* $Y$ is (100 x 3): this is the dense (one-hot) version of the labels, matching the shape of the predictions.\n",
    "* $X$ is (100 x 4): 4 features for each example.\n",
    "* The resulting product is (3 x 100)(100 x 4), giving a (3 x 4) output, which matches the shape of our parameters $W$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0-j0soKK2qfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient:\n",
      " [[ 0.337 -0.28   0.431  0.411]\n",
      " [-0.042  0.191 -0.089 -0.046]\n",
      " [-0.295  0.09  -0.342 -0.365]]\n"
     ]
    }
   ],
   "source": [
    "# y' = [.1, .2, .7]  y = [0, 0, 1]  diff = y' - y = [.1, .2, -.3]\n",
    "# d1 = [.1, .2, -.3]  x1 = [1, 2, 3, 4]\n",
    "# (3 x 100) (100 x 4) -> (3 x 4)\n",
    "# [ [ .1*1,  .1*2,  .1*3,  .1*4 ]\n",
    "#   [ .2*1,  .2*2,  .2*3,  .2*4 ]\n",
    "#   [-.3*1, -.3*2, -.3*3, -.3*4 ]\n",
    "# ]\n",
    "#\n",
    "# We need the dense version of Y here\n",
    "m = Y_train.shape[0]\n",
    "Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n",
    "diff = preds - Y_train_dense\n",
    "gradient = np.dot(diff.T, X_train) / m\n",
    "print('gradient:\\n', gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ExL4G-pMAXvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.333]\n",
      " [ 0.333]\n",
      " [-0.667]]\n",
      "[[-0.017 -0.543  0.76   1.567]]\n",
      "gradient:\n",
      " [[-0.006 -0.181  0.253  0.522]\n",
      " [-0.006 -0.181  0.253  0.522]\n",
      " [ 0.011  0.362 -0.507 -1.045]]\n"
     ]
    }
   ],
   "source": [
    "# Simplify and just compute the gradient for the first training example.\n",
    "print(diff[0:1].T)\n",
    "print(X_train[0:1])\n",
    "print('gradient:\\n', np.dot(diff[0:1].T, X_train[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZDyrbc42rcF"
   },
   "source": [
    "## Running Gradient Descent\n",
    "\n",
    "Let's put together the code for a single gradient descent step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Zl_Nu_wB8ar4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      " [2 1 0 2 0 2]\n",
      "predictions:\n",
      " [[0.025 0.201 0.774]\n",
      " [0.084 0.673 0.243]\n",
      " [0.99  0.006 0.003]\n",
      " [0.007 0.154 0.838]\n",
      " [0.962 0.032 0.006]\n",
      " [0.014 0.081 0.905]]\n",
      "loss: 0.43657251861677077\n",
      "gradient:\n",
      " [[ 0.012 -0.026  0.026  0.023]\n",
      " [-0.01   0.026 -0.007  0.01 ]\n",
      " [-0.001 -0.    -0.018 -0.033]]\n",
      "weights:\n",
      " [[0.539 1.556 0.295 0.347]\n",
      " [1.08  0.492 1.132 0.922]\n",
      " [1.381 0.951 1.573 1.731]]\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "m, n = X.shape  # m = number of examples; n = number of features (including bias)\n",
    "learning_rate = 0.01\n",
    "\n",
    "for _ in range(1000):\n",
    "  preds = softmax(np.dot(X_train, W.T))\n",
    "  loss = ce_loss(preds, Y_train)\n",
    "  gradient = np.dot((preds - tf.keras.utils.to_categorical(Y_train)).T, X_train) / m\n",
    "  W = W - learning_rate * gradient\n",
    "\n",
    "print('labels:\\n', Y_train[:6])\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('loss:', loss)\n",
    "print('gradient:\\n', gradient)\n",
    "print('weights:\\n', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q72Tu_n_LlO"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tj3z7t6-_PZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_preds = softmax(np.dot(X_test, W.T))\n",
    "test_pred_labels = np.argmax(test_preds, axis=1)\n",
    "print('Accuracy:', np.mean(test_pred_labels == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xcq8zqKDALmC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = tf.math.confusion_matrix(Y_test, test_pred_labels)\n",
    "ax = sns.heatmap(cf, annot=True, fmt='.3g', cmap='Blues',\n",
    "                 xticklabels=class_names, yticklabels=class_names, cbar=False)\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMsSJ-ZD_12e"
   },
   "source": [
    "## Now with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jisaFtGY__KL"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=3,                     # output dim\n",
    "    input_shape=[4],             # input dim\n",
    "    use_bias=True,              # we included the bias in X\n",
    "    activation='softmax',        # apply a sigmoid to the output\n",
    "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
    "))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3PQ-RDwXCKVt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "loss: 0.9904299378395081\n",
      "W:\n",
      " [[-0.752  2.519 -1.254 -1.258]\n",
      " [ 1.902  0.546  0.74  -0.596]\n",
      " [ 1.731 -0.141  3.473  3.868]]\n"
     ]
    }
   ],
   "source": [
    "# As above, get predictions for the current model first.\n",
    "preds = model.predict(X)\n",
    "\n",
    "# Do a single gradient update.\n",
    "history = model.fit(\n",
    "  x = X_train,\n",
    "  y = Y_train,\n",
    "  epochs=100,\n",
    "  batch_size=10,\n",
    "  verbose=0)\n",
    "\n",
    "# Show the loss (before the update) and the new weights.\n",
    "loss = history.history['loss'][0]\n",
    "weights = model.layers[0].get_weights()[0].T\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('loss:', loss)\n",
    "print('W:\\n', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QAmb6PMCTVET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9904299378395081, 0.8023491501808167, 0.6743261218070984, 0.5960136651992798, 0.5435795783996582, 0.5055320262908936, 0.4775417447090149, 0.455306738615036, 0.43560925126075745, 0.41975194215774536, 0.405414879322052, 0.39325374364852905, 0.3812800347805023, 0.37085410952568054, 0.361191064119339, 0.3516416549682617, 0.3428919315338135, 0.3344845473766327, 0.32639172673225403, 0.31937798857688904, 0.31135764718055725, 0.30408912897109985, 0.2973287105560303, 0.29149770736694336, 0.2847835421562195, 0.27878323197364807, 0.2733699381351471, 0.26772114634513855, 0.26227042078971863, 0.2570343613624573, 0.2525287866592407, 0.24727044999599457, 0.24270758032798767, 0.23826836049556732, 0.23427504301071167, 0.22960232198238373, 0.22551488876342773, 0.22198396921157837, 0.2186042219400406, 0.21470990777015686, 0.21076197922229767, 0.20784766972064972, 0.2046259343624115, 0.20098243653774261, 0.19852465391159058, 0.1949581354856491, 0.19184249639511108, 0.1893777996301651, 0.18643994629383087, 0.1838422268629074, 0.1811840981245041, 0.17859971523284912, 0.1764543652534485, 0.17433500289916992, 0.1715078353881836, 0.16928988695144653, 0.16704285144805908, 0.16524840891361237, 0.1631125956773758, 0.16098447144031525, 0.15904143452644348, 0.15718257427215576, 0.1553926318883896, 0.15366536378860474, 0.15207724273204803, 0.15013617277145386, 0.14851833879947662, 0.14671719074249268, 0.14597679674625397, 0.14388655126094818, 0.14222604036331177, 0.1411101520061493, 0.13946674764156342, 0.13783331215381622, 0.13679727911949158, 0.1352270245552063, 0.13427995145320892, 0.13247068226337433, 0.13148082792758942, 0.13007690012454987, 0.12925037741661072, 0.12767565250396729, 0.12668201327323914, 0.12579861283302307, 0.12437877058982849, 0.12319450080394745, 0.12242520600557327, 0.12131914496421814, 0.1202738881111145, 0.11916202306747437, 0.1183210015296936, 0.11757639795541763, 0.11625522375106812, 0.11584238708019257, 0.11448972672224045, 0.1136372983455658, 0.11286844313144684, 0.1118844673037529, 0.11121896654367447, 0.1102944165468216]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DuKK7l4fTktl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09462309628725052"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "test_preds_labels = np.argmax(test_preds, axis=1)\n",
    "accuracy = np.mean(test_preds_labels == Y_test)\n",
    "print(accuracy)\n",
    "model.evaluate(x=X_test, y=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOF/SzfGqGLnE58b8QnQuUU",
   "name": "03 Multiclass Logistic Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
